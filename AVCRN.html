<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Improving Visual Speech Enhancement Network by Learning Audio-visual Affinity with Multi-head Attention</title>
    <link href="../../css/bootstrap.min.css" rel="stylesheet">
  </head>
  <style type="text/css">
    main {
    padding-bottom: 50px;
    }
    header a:link {
    color: #EEEEEE;    
    }
    header a:hover {
    color: #EEEEEE;
    }
    header a:visited {
    color: #EEEEEE;
    }
    header a:active {
    color: #EEEEEE;
    }
    footer {
    padding-top: 10px;
    padding-bottom: 10px;
    background-color: #333333;
    color: #BBBBBB;
    }
    table {
    width: 100%;
    table-layout: fixed;
    }
    th {
    text-align: center;
    vertical-align: middle;
    }
    td {
    text-align: center;
    vertical-align: middle;
    }
    th#strong{
    color: #FF33CC;
    }
    #pinkstrong{
    color: #FF33CC;
    }
    audio {
    width: 100%;
    }
    .jumbotroncolor {
    background-color: #333333;
    color: #EEEEEE;
    }
    .thumbnailsize {
    width: 200px;
    }
    .thumbnailshadow {
    filter: drop-shadow(5px 5px 5px #aaa);
    }
    @media screen and (max-width:767px) {
    h1 { font-size:20pt;}
    }
    @media screen and (min-width:768px){
    h1 { font-size:30pt;}
    }
  </style>

  <body>

    <header class="header" id="top">
      <div class="jumbotron jumbotroncolor text-center">
	<div class="container">
	  <h2><p style="text-align:center">Improving Visual Speech Enhancement Network by Learning Audio-visual Affinity with Multi-head Attention</p></h2>
	  
	  <p class="lead">
		<p style="text-align:center">
	    <a>Xinmeng Xu</a>&nbsp;&nbsp;&nbsp;
		<a>Yang Wang</a>&nbsp;&nbsp;&nbsp;\
		<a>Jie Jia</a>&nbsp;&nbsp;&nbsp;
		<a>Binbin Chen</a>&nbsp;&nbsp;&nbsp;
		<a>Dejun Li</a>&nbsp;&nbsp;&nbsp;
		</p>
		<p style="text-align:center">
		Trinity College Dublin, Ireland<br>
	        vivo AI Lab, China<br>
		HBUCM, P.R. China
		</p>
	  </p>
	  <p class="lead">
	    <p style="text-align:center">Submitted to Interspeech 2022</p><br>
	  </p>
	</div>
      </div>
    </header>

    <main>            
      <div class="container">
	<hr>
	
	<div class="row" id="introduction">
	  <div class="col-md-12">
	    <h2>Improving Visual Speech Enhancement Network by Learning Audio-visual Affinity with Multi-head Attention</h2>
	    <p>
		    Audio-visual speech enhancement system is regarded to be one of promising solutions for isolating and enhancing 
		    speech of desired speaker. Conventional methods focus on predicting clean speech spectrum via a naive convolution 
		    neural network based encoder-decoder architecture, and these methods a) are not adequate to use data fully, b) are 
		    unable to effectively balance audio-visual features. The proposed model addresses these drawbacks, by a) applying a 
		    model that fuses audio and visual features layer by layer in encoding phase, and that feeds fused audio-visual features 
		    to each corresponding decoder layer, and more importantly, b) introducing a 2-stage multi-head cross attention (MHCA) 
		    mechanism to infer audio-visual speech enhancement for balancing the fused audio-visual features and eliminating irrelevant 
		    features. This paper proposes attentional audio-visual multi-layer feature fusion model, in which MHCA units are applied 
		    on feature mapping at every layer of decoder. The proposed model demonstrates the superior performance of the network 
		    against the state-of-the-art models.
	    </p>

	    <figure class="figure">				
		<center>
		  <a name="fig1"><img src="image1/model.png" class="figure-img" width="80%" alt="network"></a>
		
		</center>
	      </figure>
	  </div>
	</div>

	<hr>
	
	<div class="row" id="results">
	  <div class="col-md-12">
	    <h3>Dataset</h3>
	    <ul>
		  <li>We used <a href="http://sigmedia.tcd.ie/TCDTIMIT">the TCD-TIMIT dataset</a> and <a href="http://spandh.dcs.shef.ac.uk/avlombard/"> GRID Corpus dataset</a>.</li> 
		


	    </ul>
	    
	    
	    <hr>
		<h4><a name="exp1">Experiment Results</a></h4>
	    
	    <p class="lead"> Male speech + ambient noise</p>
	    <div>
	      <table class="table">
		<thead>
		  <tr>
		    <th></th>
		    <th>Noisy</th>
		    <th>Enhanced-AVCRN</a></th>
			<th>Enhanced-MHCA-AVCRN</th>
		  </tr>
		</thead>
		<tbody>
		  <tr>
		    <th scope="row">Sample</th>
		    <td>
		      <video controls>
			<source src="video1/n_m_m.mp4" type="video/mp4" />
		      </video>
		    </td>
		    <td>
		      <video controls>
			<source src="video1/n_m_mf.mp4" type="video/mp4" />
		      </video>
		    </td>
		    <td>
		      <video controls>
			<source src="video1/n_m_am.mp4" type="video/mp4" />
		      </video>
		    </td>
		  </tr>
		</tbody>
	      </table>
	    </div>

	    <p class="lead"> Female speech + ambient noise</p>
	    <div>
	      <table class="table">
		<thead>
		  <tr>
		    <th></th>
		    <th>Noisy</th>
		    <th>Enhanced-AVCRN</a></th>
			<th>Enhanced-MHCA-AVCRN</th>
		  </tr>
		</thead>
		<tbody>
		  <tr>
		    <th scope="row">Sample</th>
		    <td>
		      <video controls>
			<source src="video1/n_f_m.mp4" type="video/mp4" />
		      </video>
		    </td>
		    <td>
		      <video controls>
			<source src="video1/n_f_mf.mp4" type="video/mp4" />
		      </video>
		    </td>
		    <td>
		      <video controls>
			<source src="video1/n_f_am.mp4" type="video/mp4" />
		      </video>
		    </td>
		  </tr>
		</tbody>
	      </table>
	    </div>

	    <p class="lead">Female speech + unknown talker speech</p>
	    <div>
	      <table class="table">
		<thead>
		  <tr>
		    <th></th>
		    <th>Mixture</th>
		    <th>Enhanced-AVCRN</a></th>
			<th>Enhanced-MHCA-AVCRN</th>
		  </tr>
		</thead>
		<tbody>
		  <tr>
		    <th scope="row">Sample</th>
		    <td>
		      <video controls>
			<source src="video1/s_f_m.mp4" type="video/mp4" />
		      </video>
		    </td>
			<td>
		      <video controls>
			<source src="video1/s_f_mf.mp4" type="video/mp4" />
		      </video>
		    </td>
		    <td>
		      <video controls>
			<source src="video1/s_f_am.mp4" type="video/mp4" />
		      </video>
		    </td>
		  </tr>
		</tbody>
	      </table>
	    </div>

		 
	    <p class="lead">Male speech + unknown talker speech</p>
	   <div>
	      <table class="table">
		<thead>
		  <tr>
		    <th></th>
		    <th>Mixture</th>
			<th>Enhanced-AVCRN</a></th>
		    <th>Enhanced-MHCA-AVCRN</th>
		  </tr>
		</thead>
		<tbody>
		  <tr>
		    <th scope="row">Sample</th>
		    <td>
		      <video controls>
			<source src="video1/s_m_m.mp4" type="video/mp4" />
		      </video>
		    </td>
			<td>
		      <video controls>
			<source src="video1/s_m_mf.mp4" type="video/mp4" />
		      </video>
		    </td>
		    <td>
		      <video controls>
			<source src="video1/s_m_am.mp4" type="video/mp4" />
		      </video>
		    </td>
		  </tr>
		</tbody>
	      </table>
	    </div>
		<hr>
	  </div>
	</div>
	<hr>
	
	


	<hr>
	<strong>NOTE:</strong>
	<ul>
		<li>If you want to cite this paper, try this:
			Xinmeng Xu, Yang Wang, Jie Jia, Binbin Chen, Dejun Li  "Improving Visual Speech Enhancement Network by Learning Audio-visual Affinity with Multi-head Attention"</li>
	</ul>
      </div>
    </main>
    <script src="../../js/bootstrap.min.js"></script>
  </body>
</html>
