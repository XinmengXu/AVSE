<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Multi-stage Progressive Speech Enhancement Network</title>
    <link href="../../css/bootstrap.min.css" rel="stylesheet">
  </head>
  <style type="text/css">
    main {
    padding-bottom: 50px;
    }
    header a:link {
    color: #EEEEEE;    
    }
    header a:hover {
    color: #EEEEEE;
    }
    header a:visited {
    color: #EEEEEE;
    }
    header a:active {
    color: #EEEEEE;
    }
    footer {
    padding-top: 10px;
    padding-bottom: 10px;
    background-color: #333333;
    color: #BBBBBB;
    }
    table {
    width: 100%;
    table-layout: fixed;
    }
    th {
    text-align: center;
    vertical-align: middle;
    }
    td {
    text-align: center;
    vertical-align: middle;
    }
    th#strong{
    color: #FF33CC;
    }
    #pinkstrong{
    color: #FF33CC;
    }
    audio {
    width: 100%;
    }
    .jumbotroncolor {
    background-color: #333333;
    color: #EEEEEE;
    }
    .thumbnailsize {
    width: 200px;
    }
    .thumbnailshadow {
    filter: drop-shadow(5px 5px 5px #aaa);
    }
    @media screen and (max-width:767px) {
    h1 { font-size:20pt;}
    }
    @media screen and (min-width:768px){
    h1 { font-size:30pt;}
    }
  </style>

  <body>

    <header class="header" id="top">
      <div class="jumbotron jumbotroncolor text-center">
	<div class="container">
	  <h2><p style="text-align:center">Multi-stage Progressive Speech Enhancement Network</p></h2>
	  
	  <p class="lead">
		<p style="text-align:center">
	    <a>Xinmeng Xu,</a>&nbsp;&nbsp;&nbsp;
		<a>Yang Wang,</a>&nbsp;&nbsp;&nbsp;
	    <a>Dongxiang Xu,</a>&nbsp;&nbsp;&nbsp;
		<a>Cong Zhang,</a>&nbsp;&nbsp;&nbsp;
		<a>Yiyuan Peng,</a>&nbsp;&nbsp;&nbsp;
		<a>Jie Jia,</a>&nbsp;&nbsp;&nbsp;
		<a>Binbin Chen</a>&nbsp;&nbsp;&nbsp;
		</p>
		<p style="text-align:center">
		vivo AI Lab, P.R. China<br>
		Trinity College Dublin, Ireland
		</p>
	  </p>
	  <p class="lead">
	    <p style="text-align:center">Accepted by Interspeech2021</p><br>
	  </p>
	</div>
      </div>
    </header>

    <main>            
      <div class="container">
	<hr>
	
	<div class="row" id="introduction">
	  <div class="col-md-12">
	    <h2>Multi-stage Progressive Speech Enhancement Network</h2>
	    <p>
			Speech enhancement is a fundamental way to separate and generate clean speech from 
			adverse environment where the received speech is seriously corrupted by noise. This 
			paper applies a novel progressive network for speech enhancement by using multi-stage 
			structure, where each stage contains a Channel Attention (CA) block followed by dilated 
			encoder-decoder convolutionalnetworkwithgatedlinearunits. Inaddition,each stage generates 
			a prediction that is reﬁned by a supervised attention block. What is more, a fusion block 
			is inserted between original inputs and outputs of previous stage. Multi-stage architecture 
			is introduced to sequentially invoke multiple deeplearning networks and its key ingredient 
			is the information exchange between different stages. By doing so, a more ﬂexible and robust 
			outputs can be generated. Experimental results show that proposed architecture obtains 
			consistently better performance than recent state-of-art models in terms of both PESQ and STOI scores.
	    </p>

	    <figure class="figure">				
		<center>
		  <a name="fig1"><img src="image/mpn.jpg" class="figure-img" width="40%" alt="Figure 1: Schematic diagram of three-stage architecture for multi-stageprogressivespeechenhancement. TheConvandCat represent the convolution layer and concatenate layer, respectively. CA denotes the channel attention block, and its structure is shown in Figure 2(d). SA is supervised attention block that diagram block is shown in Figure 2(c). Red arrows represent the cross-stage feature fusion and its diagram block is shown in Figure 2(b). In addition, the dilated encoder-decoder convolutional networks with GLUs (see Figure 2(a)) framed by gray dotted box.
 "></a>	
		  <a name="fig2"><img src="image/detail.jpg" class="figure-img" width="40%" alt="Figure 2: (a)Block diagram of dilated encoder-decoder convolutional networks with GLUs. (b)Cross-stage feature fusion between two stages. (c) Supervised attention module. (d) Channel attention module, it computes the attention mask W and apply it to value,A is the attention component.
"></a>	
		</center>
	      </figure>
	  </div>
	</div>

	<hr>
	
	<div class="row" id="results">
	  <div class="col-md-12">
	    <h3>Dataset</h3>
	    <ul>
		  <li>This model trained by TIMIT dataset, and tested by TIMIT dataset and AVSpeech dataset</a>.</li> 
		


	    </ul>
	    
	    
	    <hr>
		<h4><a name="exp1">Experiment Results</a></h4>
	    
	    <p class="lead">A speech enhancement sample. This sample tested by the speech of Johnson (Male), and May (Female) with adding ambient noise.</p>
	    <div>
	      <table class="table" align="right" width="1100">
		<thead>
		  <tr>
		    <th>Sample of Johnson</th>
			<th>Sample of May</a></th>
		  </tr>
		</thead>
		<tbody>
		  <tr>
		    <td width="45%">
		      <video width="500" height="300" controls="controls">
			<source src="video/sample2.mp4" type="video/mp4">
		      </video>
		    </td>
		    <td width="45%">
		      <video width="500" height="300" controls="controls">
			<source src="video/sample3.mp4" type="video/mp4">
		      </video>
		    </td>
		  </tr>
		</tbody>
	      </table>
	    
		</div>	 
	    <p class="lead">This is a speech separation sample. We replace the branch network (Figure 2(a)) with <a href="https://XinmengXu.github.io/AVSE/MultilayerFFCN">MFFCN</a> that is an audio-visual speech enhancement model.
						The purpose of this experiment is to prove that our model can flexibly replace any network on its branches to achieve speech enhancement & separation tasks in different situations</p>
	   <div>
	      <table class="table" align="right">
		<thead>
		  <tr>
			<th>Speech separation sample</th>
		  </tr>
		</thead>
		<tbody>
		  <tr>
			<td width="80%">
		      <video width="640" height="480"  controls="controls">
			<source src="video/sample1.mp4" type="video/mp4">
		      </video>
		    </td>
		  </tr>
		</tbody>
	      </table>
	    </div>
		<hr>
	  </div>
	</div>
	<hr>
	
	


	<hr>
	<strong>NOTE:</strong>
	<ul>
		<li>If you want to cite this paper, try this:
			Xinmeng Xu, Yang Wang, Dongxiang Xu, Cong Zhang, Yiyuan Peng, Jie Jia, Binbin Chen, "Multi-stage Progressive Speech Enhancement Network"</li>
	</ul>
      </div>
    </main>
    <script src="../../js/bootstrap.min.js"></script>
  </body>
</html>
