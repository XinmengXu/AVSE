<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Attentional Multi-layer Feature Fusion Convolution Network for Audio-visual Speech Enhancement</title>
    <link href="../../css/bootstrap.min.css" rel="stylesheet">
  </head>
  <style type="text/css">
    main {
    padding-bottom: 50px;
    }
    header a:link {
    color: #EEEEEE;    
    }
    header a:hover {
    color: #EEEEEE;
    }
    header a:visited {
    color: #EEEEEE;
    }
    header a:active {
    color: #EEEEEE;
    }
    footer {
    padding-top: 10px;
    padding-bottom: 10px;
    background-color: #333333;
    color: #BBBBBB;
    }
    table {
    width: 100%;
    table-layout: fixed;
    }
    th {
    text-align: center;
    vertical-align: middle;
    }
    td {
    text-align: center;
    vertical-align: middle;
    }
    th#strong{
    color: #FF33CC;
    }
    #pinkstrong{
    color: #FF33CC;
    }
    audio {
    width: 100%;
    }
    .jumbotroncolor {
    background-color: #333333;
    color: #EEEEEE;
    }
    .thumbnailsize {
    width: 200px;
    }
    .thumbnailshadow {
    filter: drop-shadow(5px 5px 5px #aaa);
    }
    @media screen and (max-width:767px) {
    h1 { font-size:20pt;}
    }
    @media screen and (min-width:768px){
    h1 { font-size:30pt;}
    }
  </style>

  <body>

    <header class="header" id="top">
      <div class="jumbotron jumbotroncolor text-center">
	<div class="container">
	  <h2><p style="text-align:center">AMFFCN: Attentional Multi-layer Feature Fusion Convolution Network for Audio-visual Speech Enhancement</p></h2>
	  
	  <p class="lead">
		<p style="text-align:center">
	    <a>Xinmeng Xu</a>&nbsp;&nbsp;&nbsp;
		<a>Yang Wang</a>&nbsp;&nbsp;&nbsp;
	    <a>Dongxiang Xu</a>&nbsp;&nbsp;&nbsp;
		<a>Cong Zhang</a>&nbsp;&nbsp;&nbsp;
		<a>Yiyuan Peng</a>&nbsp;&nbsp;&nbsp;
		<a>Jie Jia</a>&nbsp;&nbsp;&nbsp;
		<a>Binbin Chen</a>&nbsp;&nbsp;&nbsp;
		</p>
		<p style="text-align:center">
		Trinity College Dublin, Ireland<br>
		The First Department of AI, vivo, P.R. China
		</p>
	  </p>
	  <p class="lead">
	    <p style="text-align:center">Submitted to EUSIPCO 2021</p><br>
	  </p>
	</div>
      </div>
    </header>

    <main>            
      <div class="container">
	<hr>
	
	<div class="row" id="introduction">
	  <div class="col-md-12">
	    <h2>Attentional Multi-layer Feature Fusion Convolution Network</h2>
	    <p>
			Audio-visual speech enhancement system is thought to be one of promising solutions for isolating and enhancing speech of 
			desired speaker. Conventional methods focus on predicting clean speech spectrum via a naive convolution neural network 
			based encoder-decoder architecture, and these methods a) fail to use data fully and effectively, b) cannot process features 
			selectively. The proposed model addresses these drawbacks, a) by applying a model that fuses audio and visual features layer 
			by layer in encoding phase, and that feeds fused audio-visual features to each corresponding decoder layer, and more importantly, 
			b) by introducing soft threshold attention inside the model to select the informative modality softly. This paper proposes attentional
			audio-visual multi-layer feature fusion model, in which soft-threshold attention unit are applied on feature mapping at every layer 
			of decoder part. The proposed model demonstrates the superior performance of the network against the state-of-the-art approaches.
	    </p>

	    <figure class="figure">				
		<center>
		  <a name="fig1"><img src="image1/yui.png" class="figure-img" width="80%" alt="network"></a>	
		</center>
	      </figure>
	  </div>
	</div>

	<hr>
	
	<div class="row" id="results">
	  <div class="col-md-12">
	    <h3>Dataset</h3>
	    <ul>
		  <li>We used <a href="http://sigmedia.tcd.ie/TCDTIMIT">the TCD-TIMIT dataset</a> and <a href="http://spandh.dcs.shef.ac.uk/avlombard/"> GRID Corpus dataset</a>.</li> 
		


	    </ul>
	    
	    
	    <hr>
		<h4><a name="exp1">Experiment Results</a></h4>
	    
	    <p class="lead"> Male speech + ambient noise</p>
	    <div>
	      <table class="table">
		<thead>
		  <tr>
		    <th></th>
		    <th>Noisy</th>
		    <th>Enhanced-MFFCN</th>
			<th>Enhanced-AMFFCN</th>
		  </tr>
		</thead>
		<tbody>
		  <tr>
		    <th scope="row">Sample</th>
		    <td>
		      <video controls>
			<source src="video1/n_m_m.mp4" type="video/mp4" />
		      </video>
		    </td>
		    <td>
		      <video controls>
			<source src="video1/n_m_mf.mp4" type="video/mp4" />
		      </video>
		    </td>
		    <td>
		      <video controls>
			<source src="video1/n_m_am.mp4" type="video/mp4" />
		      </video>
		    </td>
		  </tr>
		</tbody>
	      </table>
	    </div>

	    <p class="lead"> Female speech + ambient noise</p>
	    <div>
	      <table class="table">
		<thead>
		  <tr>
		    <th></th>
		    <th>Noisy</th>
		    <th>Enhanced-MFFCN</th>
			<th>Enhanced-AMFFCN</th>
		  </tr>
		</thead>
		<tbody>
		  <tr>
		    <th scope="row">Sample</th>
		    <td>
		      <video controls>
			<source src="video1/n_f_m.mp4" type="video/mp4" />
		      </video>
		    </td>
		    <td>
		      <video controls>
			<source src="video1/n_f_mf.mp4" type="video/mp4" />
		      </video>
		    </td>
		    <td>
		      <video controls>
			<source src="video1/n_f_am.mp4" type="video/mp4" />
		      </video>
		    </td>
		  </tr>
		</tbody>
	      </table>
	    </div>

	    <p class="lead">Female speech + unknown talker speech</p>
	    <div>
	      <table class="table">
		<thead>
		  <tr>
		    <th></th>
		    <th>Mixture</th>
		    <th>Enhanced-MFFCN</th>
			<th>Enhanced-AMFFCN</th>
		  </tr>
		</thead>
		<tbody>
		  <tr>
		    <th scope="row">Sample</th>
		    <td>
		      <video controls>
			<source src="video1/s_f_m.mp4" type="video/mp4" />
		      </video>
		    </td>
			<td>
		      <video controls>
			<source src="video1/s_f_mf.mp4" type="video/mp4" />
		      </video>
		    </td>
		    <td>
		      <video controls>
			<source src="video1/s_f_am.mp4" type="video/mp4" />
		      </video>
		    </td>
		  </tr>
		</tbody>
	      </table>
	    </div>

		 
	    <p class="lead">Male speech + unknown talker speech</p>
	   <div>
	      <table class="table">
		<thead>
		  <tr>
		    <th></th>
		    <th>Mixture</th>
			<th>Enhanced-MFFCN</th>
		    <th>Enhanced-AMFFCN</th>
		  </tr>
		</thead>
		<tbody>
		  <tr>
		    <th scope="row">Sample</th>
		    <td>
		      <video controls>
			<source src="video1/s_m_m.mp4" type="video/mp4" />
		      </video>
		    </td>
			<td>
		      <video controls>
			<source src="video1/s_m_mf.mp4" type="video/mp4" />
		      </video>
		    </td>
		    <td>
		      <video controls>
			<source src="video1/s_m_am.mp4" type="video/mp4" />
		      </video>
		    </td>
		  </tr>
		</tbody>
	      </table>
	    </div>
		<hr>
	  </div>
	</div>
	<hr>
	
	


	<hr>
	<strong>NOTE:</strong>
	<ul>
		<li>If you want to cite this paper, try this:
			Xinmeng Xu, Yang Wang, Dongxiang Xu, Cong Zhang, Yiyuan Peng, Jie Jia, Binbin Chen, "Attentional Multi-layer Feature Fusion Convolution Network for Audio-visual Speech Enhancement" (Submitted to EUSIPCO 2021)</li>
	</ul>
      </div>
    </main>
    <script src="../../js/bootstrap.min.js"></script>
  </body>
</html>